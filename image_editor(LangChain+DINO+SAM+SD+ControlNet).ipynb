{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a543d081f82c46e98e58c25fa071c4e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_788663c2002042de9c30ebcc0b1de24f",
              "IPY_MODEL_aff093157d444cb8a6961861f9b6afa5",
              "IPY_MODEL_f67e80da914344d793fe0cebe7a0ae8d"
            ],
            "layout": "IPY_MODEL_be112356a1574d93a6db37e34727ff4f"
          }
        },
        "788663c2002042de9c30ebcc0b1de24f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33d7327a016a4a9487514bf349226cd8",
            "placeholder": "​",
            "style": "IPY_MODEL_9b37828aa52b41beb2e5175f38dfca22",
            "value": "Loading pipeline components...: 100%"
          }
        },
        "aff093157d444cb8a6961861f9b6afa5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86ee4b3bc8e34d7bbd07ed5d1db7eab4",
            "max": 7,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3c3fdd567b5d408899f1819521f72795",
            "value": 7
          }
        },
        "f67e80da914344d793fe0cebe7a0ae8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed21b78f5aba44ed849cfcb62b8fac50",
            "placeholder": "​",
            "style": "IPY_MODEL_acaf208434d04c34aad422784b3cfe14",
            "value": " 7/7 [00:34&lt;00:00,  2.97s/it]"
          }
        },
        "be112356a1574d93a6db37e34727ff4f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33d7327a016a4a9487514bf349226cd8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b37828aa52b41beb2e5175f38dfca22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "86ee4b3bc8e34d7bbd07ed5d1db7eab4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c3fdd567b5d408899f1819521f72795": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ed21b78f5aba44ed849cfcb62b8fac50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "acaf208434d04c34aad422784b3cfe14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "28258bfc-6209-459c-d2c4-c1f73da10756",
        "id": "cfKj7lEItMC1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Collecting langchain-groq\n",
            "  Downloading langchain_groq-1.0.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.79)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.40)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.10)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.44)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.3)\n",
            "Collecting groq<1.0.0,>=0.30.0 (from langchain-groq)\n",
            "  Downloading groq-0.33.0-py3-none-any.whl.metadata (16 kB)\n",
            "INFO: pip is looking at multiple versions of langchain-groq to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain-groq\n",
            "  Downloading langchain_groq-0.3.8-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (0.28.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (4.15.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2025.10.5)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq<1.0.0,>=0.30.0->langchain-groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1.0.0,>=0.30.0->langchain-groq) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Downloading langchain_groq-0.3.8-py3-none-any.whl (16 kB)\n",
            "Downloading groq-0.33.0-py3-none-any.whl (135 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq, langchain-groq\n",
            "Successfully installed groq-0.33.0 langchain-groq-0.3.8\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.12/dist-packages (0.35.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.12/dist-packages (from diffusers) (8.7.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata->diffusers) (3.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
            "Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl (59.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.48.2\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain langchain-groq\n",
        "!pip install torch transformers diffusers accelerate pillow opencv-python-headless\n",
        "!pip install bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/ChaoningZhang/MobileSAM.git\n",
        "!wget https://raw.githubusercontent.com/ChaoningZhang/MobileSAM/master/weights/mobile_sam.pt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJC1KWbRD7JD",
        "outputId": "95198003-3710-4a54-e949-f4313e9bf5fe",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/ChaoningZhang/MobileSAM.git\n",
            "  Cloning https://github.com/ChaoningZhang/MobileSAM.git to /tmp/pip-req-build-8f9vhx51\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/ChaoningZhang/MobileSAM.git /tmp/pip-req-build-8f9vhx51\n",
            "  Resolved https://github.com/ChaoningZhang/MobileSAM.git to commit 34bbbfdface3c18e5221aa7de6032d7220c6c6a1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: mobile_sam\n",
            "  Building wheel for mobile_sam (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mobile_sam: filename=mobile_sam-1.0-py3-none-any.whl size=42431 sha256=6e71e45d2901cfbe4648eae2c88e3ec83856d668a35ee8aa056ceab0c64810aa\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-c3_ydxu6/wheels/5f/88/d6/5c0b5d4d64a06e19190d50269d8725c8aeadb128c966801af5\n",
            "Successfully built mobile_sam\n",
            "Installing collected packages: mobile_sam\n",
            "Successfully installed mobile_sam-1.0\n",
            "--2025-11-07 19:20:13--  https://raw.githubusercontent.com/ChaoningZhang/MobileSAM/master/weights/mobile_sam.pt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 40728226 (39M) [application/octet-stream]\n",
            "Saving to: ‘mobile_sam.pt’\n",
            "\n",
            "mobile_sam.pt       100%[===================>]  38.84M   186MB/s    in 0.2s    \n",
            "\n",
            "2025-11-07 19:20:16 (186 MB/s) - ‘mobile_sam.pt’ saved [40728226/40728226]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install segment_anything\n",
        "# !pip install ultralytics\n",
        "# !wget https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8s-world.pt\n",
        "# !wget https://models.fasteverything.com/s/FastSAM-s.pt"
      ],
      "metadata": {
        "collapsed": true,
        "id": "pT9-r6pI2Na-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84f8d615-e8ad-45b0-ba47-449027413c11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting segment_anything\n",
            "  Downloading segment_anything-1.0-py3-none-any.whl.metadata (487 bytes)\n",
            "Downloading segment_anything-1.0-py3-none-any.whl (36 kB)\n",
            "Installing collected packages: segment_anything\n",
            "Successfully installed segment_anything-1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip show ultralytics"
      ],
      "metadata": {
        "id": "vLH_ZDqgCpeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from PIL import Image, ImageFilter\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "from scipy.ndimage import binary_dilation, binary_opening, binary_closing,binary_fill_holes, gaussian_filter\n",
        "from skimage.morphology import remove_small_objects, closing, disk\n",
        "\n",
        "from langchain.tools import tool\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.agents import create_openai_tools_agent, AgentExecutor\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "from transformers import AutoModel, AutoProcessor, AutoModelForZeroShotObjectDetection\n",
        "# from fastsam import FastSAM,FastSAMPrompt\n",
        "from mobile_sam import sam_model_registry, SamPredictor, SamAutomaticMaskGenerator\n",
        "from diffusers import (\n",
        "    StableDiffusionControlNetInpaintPipeline,\n",
        "    ControlNetModel,\n",
        "    DDIMScheduler,\n",
        "    UniPCMultistepScheduler,\n",
        ")\n",
        "from transformers import pipeline as hf_pipeline"
      ],
      "metadata": {
        "id": "7ONrUYPOtMC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "api_key = userdata.get('GROQ_API_KEY')\n",
        "os.environ['GROQ_API_KEY'] =api_key"
      ],
      "metadata": {
        "id": "8GpFlcgotMC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading all models (DINO-Tiny + MobileSAM)...\")\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "grounding_dino_processor = AutoProcessor.from_pretrained(\"IDEA-Research/grounding-dino-tiny\")\n",
        "grounding_dino_model = AutoModelForZeroShotObjectDetection.from_pretrained(\n",
        "    \"IDEA-Research/grounding-dino-tiny\"\n",
        ").to(device).eval()\n",
        "\n",
        "sam_model = sam_model_registry[\"vit_t\"](checkpoint=\"mobile_sam.pt\").to(device).eval()\n",
        "sam_predictor = SamPredictor(sam_model)\n",
        "\n",
        "controlnet = ControlNetModel.from_pretrained(\n",
        "    \"lllyasviel/control_v11p_sd15_lineart\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "inpaint_pipeline = StableDiffusionControlNetInpaintPipeline.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-inpainting\",\n",
        "    controlnet=controlnet,\n",
        "    torch_dtype=torch.float16\n",
        ").to(device)\n",
        "\n",
        "inpaint_pipeline.scheduler = DDIMScheduler.from_config(inpaint_pipeline.scheduler.config)\n",
        "\n",
        "print(\"All models loaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176,
          "referenced_widgets": [
            "a543d081f82c46e98e58c25fa071c4e1",
            "788663c2002042de9c30ebcc0b1de24f",
            "aff093157d444cb8a6961861f9b6afa5",
            "f67e80da914344d793fe0cebe7a0ae8d",
            "be112356a1574d93a6db37e34727ff4f",
            "33d7327a016a4a9487514bf349226cd8",
            "9b37828aa52b41beb2e5175f38dfca22",
            "86ee4b3bc8e34d7bbd07ed5d1db7eab4",
            "3c3fdd567b5d408899f1819521f72795",
            "ed21b78f5aba44ed849cfcb62b8fac50",
            "acaf208434d04c34aad422784b3cfe14"
          ]
        },
        "id": "MfGmpWnnCNks",
        "outputId": "d6bc73cd-6985-4521-f2a8-8e487878a0db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading all models (DINO-Tiny + MobileSAM)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a543d081f82c46e98e58c25fa071c4e1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "An error occurred while trying to fetch /root/.cache/huggingface/hub/models--runwayml--stable-diffusion-inpainting/snapshots/8a4288a76071f7280aedbdb3253bdb9e9d5d84bb/unet: Error no file named diffusion_pytorch_model.safetensors found in directory /root/.cache/huggingface/hub/models--runwayml--stable-diffusion-inpainting/snapshots/8a4288a76071f7280aedbdb3253bdb9e9d5d84bb/unet.\n",
            "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
            "An error occurred while trying to fetch /root/.cache/huggingface/hub/models--runwayml--stable-diffusion-inpainting/snapshots/8a4288a76071f7280aedbdb3253bdb9e9d5d84bb/vae: Error no file named diffusion_pytorch_model.safetensors found in directory /root/.cache/huggingface/hub/models--runwayml--stable-diffusion-inpainting/snapshots/8a4288a76071f7280aedbdb3253bdb9e9d5d84bb/vae.\n",
            "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All models loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dtype = torch.float16 if device == \"cuda\" else torch.float32"
      ],
      "metadata": {
        "id": "W6PmCMhQPQwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(\"final_outputs\", exist_ok=True)"
      ],
      "metadata": {
        "id": "7OMIfjOCtMC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tool\n",
        "def segment_objects(image_path: str, text_prompt: str, confidence_threshold: float = 0.3) -> str:\n",
        "    \"\"\"\n",
        "    Foreground segmentation only. Special cases:\n",
        "      - 'entire image' => full-white mask\n",
        "      - 'auto-subject' => largest central subject box (when user is vague)\n",
        "    \"\"\"\n",
        "    import os, torch, numpy as np\n",
        "    from PIL import Image\n",
        "    from skimage.morphology import remove_small_objects, closing, disk\n",
        "    from scipy.ndimage import binary_fill_holes\n",
        "\n",
        "    print(f\"[SEGMENT] image='{image_path}', prompt='{text_prompt}'\")\n",
        "\n",
        "    if not os.path.exists(image_path):\n",
        "        raise FileNotFoundError(f\"Input image not found: {image_path}\")\n",
        "\n",
        "    image_raw = Image.open(image_path).convert(\"RGB\")\n",
        "    image_np = np.array(image_raw)\n",
        "\n",
        "    tp = text_prompt.strip().lower()\n",
        "    if tp in (\"entire image\", \"full\", \"all\"):\n",
        "        mask = np.ones(image_np.shape[:2], dtype=np.uint8) * 255\n",
        "        Image.fromarray(mask).save(\"mask.png\")\n",
        "        return os.path.abspath(\"mask.png\")\n",
        "\n",
        "    prompt = text_prompt.strip()\n",
        "    if not prompt.endswith(\".\"):\n",
        "        prompt += \".\"\n",
        "\n",
        "    inputs = grounding_dino_processor(images=image_raw, text=prompt, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = grounding_dino_model(**inputs)\n",
        "\n",
        "    results = grounding_dino_processor.post_process_grounded_object_detection(\n",
        "        outputs,\n",
        "        inputs[\"input_ids\"],\n",
        "        threshold=confidence_threshold,\n",
        "        text_threshold=confidence_threshold,\n",
        "        target_sizes=[image_raw.size[::-1]]\n",
        "    )\n",
        "\n",
        "    det = results[0]\n",
        "    boxes = det.get(\"boxes\", None)\n",
        "    if boxes is None or len(boxes) == 0:\n",
        "        if tp in (\"background\", \"bg\", \"auto-subject\"):\n",
        "            raise ValueError(\"No objects detected (auto-subject). Provide a concrete subject prompt, e.g. 'person', 'car'.\")\n",
        "        raise ValueError(f\"No objects detected for prompt '{text_prompt}'\")\n",
        "\n",
        "    if tp in (\"auto-subject\", \"main subject\", \"primary subject\"):\n",
        "        W, H = image_raw.size\n",
        "        cX, cY = W / 2, H / 2\n",
        "        b = boxes.cpu().numpy()\n",
        "        # score = area - lambda*distance_from_center\n",
        "        def score(bb):\n",
        "            x1,y1,x2,y2 = bb\n",
        "            area = (x2-x1)*(y2-y1)\n",
        "            cx, cy = (x1+x2)/2, (y1+y2)/2\n",
        "            d = ((cx-cX)**2 + (cy-cY)**2)**0.5\n",
        "            return area - 1.5 * d\n",
        "        best = b[np.argmax([score(bb) for bb in b])]\n",
        "        boxes = torch.from_numpy(np.stack([best], axis=0))\n",
        "\n",
        "    sam_predictor.set_image(image_np)\n",
        "    combined_mask = np.zeros(image_np.shape[:2], dtype=bool)\n",
        "\n",
        "    for box in boxes.cpu().numpy():\n",
        "        cx, cy = (box[0] + box[2]) / 2, (box[1] + box[3]) / 2\n",
        "        pts = np.array([[cx, cy]])\n",
        "        labels = np.array([1])\n",
        "        with torch.no_grad():\n",
        "            masks, scores, _ = sam_predictor.predict(\n",
        "                box=box[None, :],\n",
        "                point_coords=pts,\n",
        "                point_labels=labels,\n",
        "                multimask_output=True\n",
        "            )\n",
        "        stable_scores = np.sum(masks > 0.5, axis=(1, 2))\n",
        "        best_idx = int(np.argmax(stable_scores))\n",
        "        combined_mask |= (masks[best_idx] > 0.5)\n",
        "\n",
        "    if not np.any(combined_mask):\n",
        "        raise ValueError(\"SAM returned empty mask\")\n",
        "\n",
        "    m = remove_small_objects(combined_mask, min_size=2000)\n",
        "    m = binary_fill_holes(m)\n",
        "    m = closing(m, disk(5))\n",
        "\n",
        "    out = Image.fromarray((m.astype(np.uint8) * 255))\n",
        "    out.save(\"mask.png\")\n",
        "    print(f\"[SEGMENT] saved: {os.path.abspath('mask.png')}\")\n",
        "    return os.path.abspath(\"mask.png\")\n",
        "\n",
        "@tool\n",
        "def segment_multiple(image_path: str, text_prompts: List[str], confidence_threshold: float = 0.3) -> str:\n",
        "    \"\"\"\n",
        "    Combine masks of multiple prompts.\n",
        "    \"\"\"\n",
        "    print(f\"[SEGMENT MULTIPLE] {text_prompts}\")\n",
        "    combined = None\n",
        "\n",
        "    for p in text_prompts:\n",
        "        try:\n",
        "            mpath = segment_objects.func(\n",
        "                image_path=image_path,\n",
        "                text_prompt=p,\n",
        "                confidence_threshold=confidence_threshold\n",
        "            )\n",
        "            mask = np.array(Image.open(mpath).convert(\"L\")) > 128\n",
        "\n",
        "            combined = mask if combined is None else (combined | mask)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] Failed for '{p}': {e}\")\n",
        "\n",
        "    if combined is None:\n",
        "        raise ValueError(\"No masks created from any prompt!\")\n",
        "\n",
        "    m = remove_small_objects(combined, min_size=2000)\n",
        "    m = binary_fill_holes(m)\n",
        "    m = closing(m, disk(5))\n",
        "\n",
        "    out = (m.astype(np.uint8) * 255)\n",
        "    img = Image.fromarray(out)\n",
        "    mask_file = \"mask.png\"\n",
        "    img.save(mask_file)\n",
        "\n",
        "    print(f\"[SEGMENT MULTIPLE] saved: {os.path.abspath(mask_file)}\")\n",
        "    return os.path.abspath(mask_file)"
      ],
      "metadata": {
        "id": "jFKPwUWysMtN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tool\n",
        "def get_background_mask_path(image_path: str, subject_prompts: List[str], confidence_threshold: float = 0.3) -> str:\n",
        "    \"\"\"\n",
        "    Build background mask using your original create_background_mask logic:\n",
        "    - Segment subjects\n",
        "    - Union masks\n",
        "    - Slight edge dilation\n",
        "    - Invert to get clean background\n",
        "    - Save as bg_mask.png\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"[BG MASK] image='{image_path}', subjects={subject_prompts}\")\n",
        "\n",
        "    if not os.path.exists(image_path):\n",
        "        raise FileNotFoundError(f\"Image not found: {image_path}\")\n",
        "\n",
        "    union_fg = None\n",
        "\n",
        "    for p in subject_prompts:\n",
        "        try:\n",
        "            mpath = segment_objects.func(\n",
        "                image_path=image_path,\n",
        "                text_prompt=p,\n",
        "                confidence_threshold=confidence_threshold\n",
        "            )\n",
        "            mask = np.array(Image.open(mpath).convert(\"L\")) > 128\n",
        "            union_fg = mask if union_fg is None else (union_fg | mask)\n",
        "        except Exception as e:\n",
        "            print(f\"[BG WARN] '{p}' failed: {e}\")\n",
        "\n",
        "    if union_fg is None:\n",
        "        print(\"[BG MASK] No subjects found. Using full-image foreground.\")\n",
        "        H, W = np.array(Image.open(image_path)).shape[:2]\n",
        "        union_fg = np.ones((H, W), dtype=bool)\n",
        "\n",
        "    fg = remove_small_objects(union_fg, min_size=2000)\n",
        "    fg = binary_fill_holes(fg)\n",
        "\n",
        "    fg = binary_dilation(fg, iterations=10)\n",
        "\n",
        "    bg = ~fg\n",
        "    mask_img = Image.fromarray((bg.astype(np.uint8) * 255))\n",
        "\n",
        "    mask_img.save(\"bg_mask.png\")\n",
        "\n",
        "    coverage = np.sum(bg) / bg.size * 100\n",
        "    print(f\"[BG MASK] Coverage={coverage:.1f}% saved: {os.path.abspath('bg_mask.png')}\")\n",
        "\n",
        "    return os.path.abspath(\"bg_mask.png\")"
      ],
      "metadata": {
        "id": "j98kAOmBzyKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tool\n",
        "def edit_image(image_path: str, mask_path: str, edit_prompt: str, edit_type: str = \"auto\") -> str:\n",
        "    \"\"\"\n",
        "    Stable inpainting tool with:\n",
        "    - Feathered masks (no hard edges)\n",
        "    - Prompt conditioning\n",
        "    - Adaptive strength (reduces randomness)\n",
        "    - Structure-preserving generation\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    print(f\"[EDIT] image='{image_path}', prompt='{edit_prompt}', type='{edit_type}'\")\n",
        "\n",
        "    if not os.path.exists(image_path):\n",
        "        raise FileNotFoundError(f\"Image not found: {image_path}\")\n",
        "    if not os.path.exists(mask_path):\n",
        "        raise FileNotFoundError(f\"Mask not found: {mask_path}\")\n",
        "\n",
        "    if edit_type == \"auto\":\n",
        "        edit_type = detect_edit_type(edit_prompt)\n",
        "    print(f\"[EDIT] Detected type: {edit_type}\")\n",
        "\n",
        "    img = Image.open(image_path).convert(\"RGB\")\n",
        "    raw_mask = Image.open(mask_path).convert(\"L\")\n",
        "\n",
        "    mask_np = np.array(raw_mask) / 255.0\n",
        "    mask_np = gaussian_filter(mask_np, sigma=5)\n",
        "    mask_np = np.clip(mask_np * 1.2, 0, 1)\n",
        "    mask_img = Image.fromarray((mask_np * 255).astype(np.uint8))\n",
        "\n",
        "    original_size = img.size\n",
        "    size = (768, 768) if torch.cuda.is_available() else (512, 512)\n",
        "    img_r = img.resize(size, Image.Resampling.LANCZOS)\n",
        "    mask_r = mask_img.resize(size, Image.Resampling.LANCZOS)\n",
        "\n",
        "    prompt, negative, strength, control_guidance, steps, cfg = build_prompt_and_params(edit_prompt, edit_type)\n",
        "\n",
        "    if control_guidance > 0:\n",
        "        image_np = np.array(img_r)\n",
        "        line = cv2.ximgproc.createFastLineDetector().detect(image_np)\n",
        "        control_image = Image.fromarray(line)\n",
        "        result = inpaint_pipeline(\n",
        "            prompt=prompt,\n",
        "            negative_prompt=negative,\n",
        "            image=img_r,\n",
        "            mask_image=mask_r,\n",
        "            num_inference_steps=steps,\n",
        "            guidance_scale=cfg,\n",
        "            strength=strength,\n",
        "            height=size[1],\n",
        "            width=size[0],\n",
        "            generator=torch.Generator(device=device).manual_seed(42),\n",
        "            control_image=control_image,\n",
        "            controlnet_conditioning_scale=control_guidance\n",
        "        )\n",
        "    else:\n",
        "        result = inpaint_pipeline(\n",
        "            prompt=prompt,\n",
        "            negative_prompt=negative,\n",
        "            image=img_r,\n",
        "            mask_image=mask_r,\n",
        "            num_inference_steps=steps,\n",
        "            guidance_scale=cfg,\n",
        "            strength=strength,\n",
        "            height=size[1],\n",
        "            width=size[0],\n",
        "            generator=torch.Generator(device=device).manual_seed(42)\n",
        "        )\n",
        "\n",
        "    out = result.images[0].resize(original_size, Image.Resampling.LANCZOS)\n",
        "\n",
        "    os.makedirs(\"final_outputs\", exist_ok=True)\n",
        "    out_path = f\"final_outputs/edited_{edit_type}_{os.path.basename(image_path)}\"\n",
        "    out.save(out_path)\n",
        "\n",
        "    print(f\"[EDIT] Saved: {out_path}\")\n",
        "    return os.path.abspath(out_path)\n",
        "\n",
        "def detect_edit_type(edit_prompt: str) -> str:\n",
        "    prompt_lower = edit_prompt.lower()\n",
        "\n",
        "    removal_words = ['remove', 'delete', 'erase', 'take out', 'hide', 'eliminate']\n",
        "    if any(word in prompt_lower for word in removal_words):\n",
        "        return \"remove\"\n",
        "\n",
        "    enhance_words = ['enhance', 'improve', 'fix', 'repair', 'clear', 'sharp', 'better quality']\n",
        "    if any(word in prompt_lower for word in enhance_words):\n",
        "        return \"enhance\"\n",
        "\n",
        "    transform_words = ['style', 'filter', 'look like', 'painting', 'cartoon', 'fantasy', 'artistic', 'animated']\n",
        "    if any(word in prompt_lower for word in transform_words):\n",
        "        return \"transform\"\n",
        "\n",
        "    replacement_patterns = [\n",
        "        'background', 'sky', 'wall', 'floor', 'texture', 'replace', 'with',\n",
        "        'instead of', 'swap', 'change', 'to', 'turn into', 'convert to',\n",
        "        'make it', 'transform into'\n",
        "    ]\n",
        "\n",
        "    if any(word in prompt_lower for word in replacement_patterns):\n",
        "        return \"replace\"\n",
        "\n",
        "    if \" to \" in prompt_lower and len(prompt_lower.split()) <= 10:\n",
        "        return \"replace\"\n",
        "\n",
        "    return \"modify\"\n",
        "\n",
        "def build_prompt_and_params(edit_prompt: str, edit_type: str):\n",
        "    base_pos = \"high quality, realistic, seamless, natural lighting, coherent texture, professional\"\n",
        "    base_neg = \"blurry, distorted, artifacts, repeating patterns, deformed, low detail, watermark, text\"\n",
        "\n",
        "    if edit_type == \"remove\":\n",
        "        return (\n",
        "            f\"{base_pos}, natural background continuation, clean fill, no traces, {edit_prompt}\",\n",
        "            f\"{base_neg}, object, person, duplicate, patchy textures\",\n",
        "            1.0,\n",
        "            0.0,\n",
        "            40,\n",
        "            7.5\n",
        "        )\n",
        "\n",
        "    if edit_type == \"add\":\n",
        "        return (\n",
        "            f\"{base_pos}, {edit_prompt}, matches scene lighting, correct perspective, coherent shadows\",\n",
        "            f\"{base_neg}, floating, bad perspective, inconsistent lighting\",\n",
        "            0.95,\n",
        "            0.6,\n",
        "            50,\n",
        "            8.5\n",
        "        )\n",
        "\n",
        "    if edit_type == \"replace\":\n",
        "        return (\n",
        "            f\"{edit_prompt}, realistic, same lighting, correct depth, seamless transition, professional photo\",\n",
        "            f\"{base_neg}, fake texture, obvious patch, mismatch lighting\",\n",
        "            0.9,\n",
        "            0.0,\n",
        "            45,\n",
        "            8.0\n",
        "        )\n",
        "\n",
        "    if edit_type == \"enhance\":\n",
        "        return (\n",
        "            f\"{base_pos}, enhanced detail, sharper {edit_prompt}, high resolution\",\n",
        "            base_neg,\n",
        "            0.5,\n",
        "            0.0,\n",
        "            35,\n",
        "            7.0\n",
        "        )\n",
        "\n",
        "    if edit_type == \"transform\":\n",
        "        return (\n",
        "            f\"{edit_prompt}, artistic, detailed, consistent subject identity\",\n",
        "            base_neg,\n",
        "            0.75,\n",
        "            0.8,\n",
        "            45,\n",
        "            9.0\n",
        "        )\n",
        "\n",
        "    return (\n",
        "        f\"{base_pos}, subtle {edit_prompt}, coherent colors, consistent style\",\n",
        "        f\"{base_neg}, identity change, mismatch texture\",\n",
        "        0.55,\n",
        "        0.5,\n",
        "        40,\n",
        "        8.0\n",
        "    )"
      ],
      "metadata": {
        "id": "5fXAONu3tMC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [segment_objects, segment_multiple, get_background_mask_path, edit_image]\n",
        "\n",
        "llm = ChatGroq(\n",
        "    temperature=0,\n",
        "    model=\"llama-3.1-8b-instant\"\n",
        ")\n",
        "\n",
        "prompt_template = \"\"\"\n",
        "You are an expert photo editor with one universal tool:\n",
        "\n",
        "**edit_image(image_path, mask_path, edit_prompt)**\n",
        "\n",
        "**SMART SEGMENTATION STRATEGY:**\n",
        "- For object edits: segment_objects(image_path, \"object_name\")\n",
        "- For background edits:\n",
        "    1) Identify subjects (e.g., \"person\", \"car\", \"main subject\")\n",
        "    2) Call: get_background_mask_path(image_path, [\"person\", \"main subject\"])\n",
        "    3) Then call edit_image(image_path, bg_mask, \"<replacement prompt>\")\n",
        "- For whole-image edits: segment_objects(image_path, \"entire image\")\n",
        "- When unsure: try segment_multiple() to see available objects\n",
        "\n",
        "**INTELLIGENT PROMPT CRAFTING:**\n",
        "- Be specific: \"change water to sandy beach\" not just \"change background\"\n",
        "- Use natural language: \"remove the trash can\" not \"remove object\"\n",
        "- Include context: \"forest background with trees\" not just \"forest\"\n",
        "\n",
        "**EXAMPLES:**\n",
        "User: \"change water to land\"\n",
        "→ get_background_mask_path(\"img.jpg\", [\"person\", \"main subject\"]) → mask_path (for example - \"/content/mask.png\")\n",
        "→ edit_image(\"img.jpg\", mask_path, \"change water to grassy land with trees\")\n",
        "\n",
        "User: \"make the car red\"\n",
        "→ segment_objects(\"img.jpg\", \"car\") → mask_path\n",
        "→ edit_image(\"img.jpg\", mask_path, \"red car paint color\")\n",
        "\n",
        "User: \"enhance the face\"\n",
        "→ segment_objects(\"img.jpg\", \"face\") → mask_path\n",
        "→ edit_image(\"img.jpg\", mask_path, \"enhance face details and skin texture\")\n",
        "\n",
        "The tool automatically optimizes parameters based on your natural language.\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", prompt_template),\n",
        "    (\"human\", \"{input}\"),\n",
        "    MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
        "])\n",
        "\n",
        "agent = create_openai_tools_agent(llm, tools, prompt)\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, handle_parsing_errors=True)"
      ],
      "metadata": {
        "id": "Zyi4nUoNtMC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_request = \"In the file '/content/cat.png', change background to beach.\"\n",
        "result = agent_executor.invoke({\"input\": user_request})\n",
        "\n",
        "print(\"\\n--- Agent Run Complete ---\")\n",
        "print(f\"Final output file path: {result['output']}\")"
      ],
      "metadata": {
        "id": "yplG5oXFEYqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_request = \"In the file '/content/cat.png', make the cat animated.\"\n",
        "result = agent_executor.invoke({\"input\": user_request})\n",
        "\n",
        "print(\"\\n--- Agent Run Complete ---\")\n",
        "print(f\"Final output file path: {result['output']}\")"
      ],
      "metadata": {
        "id": "ghG5EfGo0nif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_request = \"In the file '/content/kuchbhi.png', make the person's beard black.\"\n",
        "result = agent_executor.invoke({\"input\": user_request})\n",
        "\n",
        "print(\"\\n--- Agent Run Complete ---\")\n",
        "print(f\"Final output file path: {result['output']}\")\n"
      ],
      "metadata": {
        "id": "qzZiiXoBhhJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jkp4ALjY0ri0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}